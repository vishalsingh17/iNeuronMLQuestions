## Dimensionality Reduction

Q173. What is curse of dimensionality?

Q174. What is advantage of dimensionality reduction?

Q175. Explain the projection technique.

Q176. For what kind of dataset is the projection technique unsuitable?

Q177. What is manifold learning?

Q178. What is manifold assumption?

Q179. What is PCA (Principal Component Analysis)?

Q180. Explain how PCA reduces the dimension of the data.

Q181. What is the use of Principal Components in PCA?

Q182. How can you find the optimal number of principal components for a training set?

Q183. What is explained variance ratio?

Q184. How can you choose the right number of dimension of reduction?

Q185. How can you use PCA of data compression?

Q186. Compress the (MNIST data)[https://www.kaggle.com/datasets/oddrationale/mnist-in-csv] using PCA.

Q187. What is Randomized PCA?

Q188. What is Incremental PCA?

Q189. Explain Kernel PCA.

Q190. How can you select the best kernel and hyperparameter for dimensionality reduction?

Q191. What is LLE (Locally Linear Embedding)?

Q192. Explain the working of LLE.

Q193. What is Random Projections?

Q194. What is MDS (Multi Dimensional Scaling)?

Q195. What is Isomap?

Q196. What is t-SNE (t-Distributed Stochastic Neighbor Embedding)?

Q197. What is LDA (Linear Discriminant Analysis)?

Q198. Once a dataset’s dimensionality has been reduced, is it possible to reverse the
operation? If so, how? If not, why?

Q199. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?

Q200. Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained
variance ratio to 95%. How many dimensions will the resulting dataset have?

Q201. In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA,
or Kernel PCA?

Q202. How can you evaluate the performance of a dimensionality reduction algorithm
on your dataset?

Q203. Does it make any sense to chain two different dimensionality reduction algorithms?

Q204. Load the (MNIST data)[https://www.kaggle.com/datasets/oddrationale/mnist-in-csv] and split it into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing). Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set. Next, use PCA to reduce the dataset’s dimensionality, with an explained variance ratio of 95%. Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster? Next, evaluate the classifier on the test set. How does it compare to the previous classifier?

Q205. Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the
result using Matplotlib. You can use a scatterplot using 10 different colors to rep‐
resent each image’s target class. Alternatively, you can replace each dot in the
scatterplot with the corresponding instance’s class (a digit from 0 to 9), or even
plot scaled-down versions of the digit images themselves (if you plot all digits,
the visualization will be too cluttered, so you should either draw a random sam‐
ple or plot an instance only if no other instance has already been plotted at a
close distance). You should get a nice visualization with well-separated clusters of
digits. Try using other dimensionality reduction algorithms such as PCA, LLE, or
MDS and compare the resulting visualizations.
